# workers/scrapy_worker.py
from typing import Dict, Any
import sys

sys.path.append('/app')
from .base_worker import BaseWorker
from app.parsers.factory import ParserFactory



class ScrapyWorker(BaseWorker):
    def get_queue_name(self) -> str:
        return 'scrapy.queue'

    def process_job(self, job_data: Dict[str, Any]) -> Dict[str, Any]:
        """Scrapy job'Ä±nÄ± iÅŸle"""
        try:
            # Job verilerini al
            url = job_data['url']
            company_id = job_data['company_id']
            application_id = job_data['application_id']
            server_id = job_data['server_id']
            job_id = job_data.get('job_id')
            product_id = job_data.get('product_id')
            npm = job_data.get('npm')
            
            print(f"ğŸ“¦ Scrapy Job iÅŸleniyor - Job ID: {job_id}, Product ID: {product_id}, URL: {url}")

            # Parser'Ä± al
            parser = ParserFactory.get_parser('scrapy')

            # Parse iÅŸlemi - job_data'yÄ± da gÃ¶nder
            result = parser.parse(url, company_id, application_id, server_id, job_data=job_data)

            # BaÅŸarÄ± durumuna gÃ¶re queue'ye gÃ¶nder
            if result.get('status') == 'success':
                # âœ… Parsing baÅŸarÄ±lÄ± â†’ save.queue'ye gÃ¶nder (DB kayÄ±t iÃ§in)
                self._publish_to_save_queue(result)
                print(f"âœ… Scrapy parsing baÅŸarÄ±lÄ±, save.queue'ye gÃ¶nderildi: {url}")
            else:
                # âŒ Parsing hatasÄ± â†’ scrapy.queue.error'a gÃ¶nderilecek (base_worker halleder)
                print(f"âŒ Scrapy parsing hatasÄ±: {result.get('error')}")
            
            return result

        except Exception as e:
            print(f"âŒ Scrapy Job iÅŸleme hatasÄ±: {e}")
            import traceback
            traceback.print_exc()
            
            error_result = {
                'status': 'error',
                'error': str(e),
                'job_id': job_data.get('job_id'),
                'product_id': job_data.get('product_id'),
                'url': job_data.get('url'),
                'company_id': job_data.get('company_id'),
                'parser_used': 'scrapy'
            }
            
            return error_result


if __name__ == "__main__":
    worker = ScrapyWorker()
    worker.start_consuming()