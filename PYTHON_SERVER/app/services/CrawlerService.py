import logging
import os
import sys
import traceback
from datetime import datetime
import json

import dotenv

# Gerekli path'leri ayarla
sys.path.append('/app')
dotenv.load_dotenv('/app/.env')
server_path = os.getenv('SERVER_PATH')

if server_path and os.path.exists(server_path):
    sys.path.append(server_path)
    print("Server environment detected, using server path")

from app.database import SessionLocal, engine
from app.models import Base
from app.repositories.ImageRepository import ImageRepository
from app.repositories.ProductHistoryRepository import ProductHistoryRepository
from app.repositories.ProductRepository import ProductRepository
from app.repositories.ScreenshotRepository import ScreenshotRepository
from app.repositories.CrawlerLogRepository import CrawlerLogRepository
from app.services.PriceParser import PriceParser
from app.services.XmlParser import XmlParser
from app.services.ProductService import ProductService
from app.services.ScreenshotService import ScreenshotService
from scrapy.crawler import CrawlerProcess
from scrapy import signals

# Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('/app/crawler_subprocess.log')
    ]
)
logger = logging.getLogger(__name__)


def main():
    """Crawl iÅŸlemini gerÃ§ekleÅŸtiren ana fonksiyon - BOTH CRAWLERS"""
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    logger.info(f"ğŸš€ Crawler subprocess baÅŸlatÄ±ldÄ±... Zaman: {current_time}")

    db_session = None
    crawler_log_repo = None
    xml_parser_log_id = None
    price_parser_log_id = None

    try:
        # VeritabanÄ± oturumu
        db_session = SessionLocal()
        logger.info("âœ… VeritabanÄ± oturumu oluÅŸturuldu")

        # Repository'ler
        repositories = {
            'product': ProductRepository(db_session),
            'product_history': ProductHistoryRepository(db_session),
            'image': ImageRepository(db_session),
            'screenshot': ScreenshotRepository(db_session),
            'crawler_log': CrawlerLogRepository(db_session)
        }
        crawler_log_repo = repositories['crawler_log']
        logger.info("âœ… Repository'ler oluÅŸturuldu")

        # Servisler
        product_service = ProductService(
            repositories['product'],
            repositories['product_history'],
            repositories['image']
        )
        screenshot_service = ScreenshotService(repositories['screenshot'])
        logger.info("âœ… Servisler oluÅŸturuldu")

        # BOTH CRAWLERS iÃ§in log kayÄ±tlarÄ± oluÅŸtur
        xml_parser_log = crawler_log_repo.start_crawler('XmlParser')
        xml_parser_log_id = xml_parser_log.id
        logger.info(f"âœ… XmlParser log kaydÄ± oluÅŸturuldu. ID: {xml_parser_log_id}")

        price_parser_log = crawler_log_repo.start_crawler('PriceParser')
        price_parser_log_id = price_parser_log.id
        logger.info(f"âœ… PriceParser log kaydÄ± oluÅŸturuldu. ID: {price_parser_log_id}")

        # Crawler Process
        process = CrawlerProcess({
            'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'LOG_ENABLED': True,
            'ROBOTSTXT_OBEY': False,
            'COOKIES_ENABLED': True,
            'KEEP_ALIVE': True,
            'LOG_LEVEL': 'INFO',
        })
        logger.info("âœ… CrawlerProcess oluÅŸturuldu")

        # CALLBACK FONKSÄ°YONU - HER Ä°KÄ° CRAWLER Ä°Ã‡Ä°N
        def create_spider_callback(crawler_log_id, crawler_name):
            def spider_closed_callback(spider, reason):
                try:
                    logger.info(f"=== ğŸ•·ï¸ {crawler_name} CLOSED CALLBACK Ã‡ALIÅTI ===")
                    logger.info(f"Spider: {spider.name}, Reason: {reason}")

                    # Scrapy'nin kendi istatistiklerini al
                    stats = spider.crawler.stats.get_stats()
                    logger.info(f"ğŸ“Š Stats alÄ±ndÄ±: {len(stats)} item")

                    # Spider'dan temel sayaÃ§larÄ± al
                    xml_urls_count = getattr(spider, 'xml_urls_count', 0)
                    product_urls_count = getattr(spider, 'product_urls_count', 0)
                    total_urls_visited = getattr(spider, 'total_urls_visited', 0)
                    processed_products_count = getattr(spider, 'processed_products_count', 0)

                    # â† YENÄ°: Spider'dan detaylÄ± stats'i al
                    url_statuses = getattr(spider, 'url_statuses', [])
                    companies_processed = getattr(spider, 'companies_processed', set())
                    status_404_count = getattr(spider, 'status_404_count', 0)

                    # â† YENÄ°: Status summary ve error details hesapla
                    status_summary = {}
                    error_details = []

                    for url_status in url_statuses:
                        status_code = str(url_status['status'])
                        if status_code not in status_summary:
                            status_summary[status_code] = 0
                        status_summary[status_code] += 1

                        if url_status['status'] != 200:
                            error_details.append(url_status)

                    # Temel crawler stats
                    crawler_stats = {
                        'processed': stats.get('response_received_count', 0),
                        'updated': product_urls_count,
                        'created': processed_products_count,
                        'downloader/request_count': stats.get('downloader/request_count', 0),
                        'downloader/response_count': stats.get('downloader/response_count', 0),
                        'downloader/response_status_count/200': stats.get('downloader/response_status_count/200', 0),
                        'downloader/response_status_count/404': stats.get('downloader/response_status_count/404', 0),
                        'downloader/exception_count': stats.get('downloader/exception_count', 0),
                        'log_count/ERROR': stats.get('log_count/ERROR', 0),
                        'log_count/WARNING': stats.get('log_count/WARNING', 0),
                        'log_count/INFO': stats.get('log_count/INFO', 0),
                        'elapsed_time_seconds': stats.get('elapsed_time_seconds', 0),

                        # CUSTOM URL STATÄ°STÄ°KLERÄ°
                        'custom/xml_urls_count': xml_urls_count,
                        'custom/product_urls_count': product_urls_count,
                        'custom/total_urls_visited': total_urls_visited,
                        'custom/processed_products_count': processed_products_count,

                        # â† YENÄ°: DetaylÄ± stats
                        'custom/companies_processed': len(companies_processed),
                        'custom/url_status_summary': status_summary,
                        'custom/error_details': error_details,
                        'custom/total_errors': len(error_details),
                        'custom/status_404_count': status_404_count,
                    }

                    # â† YENÄ°: Debug bilgileri
                    logger.info(f"ğŸ” CALLBACK DEBUG:")
                    logger.info(f"   - url_statuses boyutu: {len(url_statuses)}")
                    logger.info(f"   - error_details boyutu: {len(error_details)}")
                    logger.info(f"   - status_summary: {status_summary}")
                    logger.info(f"   - companies_processed: {len(companies_processed)}")

                    # Mevcut istatistik loglarÄ±...
                    logger.info(f"===== ğŸ“ˆ {crawler_name} Ä°STATÄ°STÄ°KLERÄ° =====")
                    logger.info(f"ğŸŒ URL Ä°statistikleri:")
                    logger.info(f"   - XML URL'leri: {xml_urls_count}")
                    logger.info(f"   - ÃœrÃ¼n URL'leri: {product_urls_count}")
                    logger.info(f"   - Toplam ziyaret edilen URL: {total_urls_visited}")
                    logger.info(f"ğŸ“¦ ÃœrÃ¼n Ä°statistikleri:")
                    logger.info(f"   - Ä°ÅŸlenen Ã¼rÃ¼n sayÄ±sÄ±: {processed_products_count}")
                    logger.info(f"   - Fiyat gÃ¼ncellenen Ã¼rÃ¼n: {product_urls_count}")
                    logger.info(f"ğŸ”§ Scrapy Ä°statistikleri:")
                    logger.info(f"   - Toplam istek: {crawler_stats['downloader/request_count']}")
                    logger.info(f"   - Toplam yanÄ±t: {crawler_stats['downloader/response_count']}")
                    logger.info(f"   - BaÅŸarÄ±lÄ± (200): {crawler_stats['downloader/response_status_count/200']}")
                    logger.info(f"   - Hata (404): {crawler_stats['downloader/response_status_count/404']}")
                    logger.info(f"â±ï¸ SÃ¼re: {crawler_stats['elapsed_time_seconds']} saniye")
                    logger.info(f"================================")

                    # VeritabanÄ±na kaydet
                    if reason == 'finished':
                        stats_json = json.dumps(crawler_stats, indent=2)

                        logger.info(f"ğŸ’¾ {crawler_name} DB gÃ¼ncelleniyor. ID: {crawler_log_id}")
                        logger.info(f"ğŸ“Š Stats JSON boyutu: {len(stats_json)} karakter")

                        result = crawler_log_repo.complete_crawler(crawler_log_id, {
                            'processed': crawler_stats['downloader/response_count'],
                            'updated': product_urls_count,
                            'created': processed_products_count,
                            'stats_json': stats_json
                        })

                        if result:
                            logger.info(f"âœ… {crawler_name} DB baÅŸarÄ±yla gÃ¼ncellendi (COMPLETED)")
                        else:
                            logger.error(f"âŒ {crawler_name} DB gÃ¼ncelleme baÅŸarÄ±sÄ±z!")

                    else:
                        logger.info(f"âŒ {crawler_name} failed ile gÃ¼ncelleniyor. Reason: {reason}")
                        result = crawler_log_repo.fail_crawler(crawler_log_id, f"Spider kapatÄ±ldÄ±: {reason}")

                        if result:
                            logger.info(f"âœ… {crawler_name} DB baÅŸarÄ±yla gÃ¼ncellendi (FAILED)")
                        else:
                            logger.error(f"âŒ {crawler_name} DB gÃ¼ncelleme baÅŸarÄ±sÄ±z!")

                except Exception as e:
                    logger.error(f"ğŸ’¥ {crawler_name} Callback hatasÄ±: {repr(e)}")
                    logger.error(f"ğŸ“‹ Traceback: {traceback.format_exc()}")

                    # Hata durumunda da DB'yi gÃ¼ncellemeye Ã§alÄ±ÅŸ
                    try:
                        crawler_log_repo.fail_crawler(crawler_log_id, f"Callback error: {str(e)}")
                        logger.info(f"âš ï¸ {crawler_name} hata durumunda DB gÃ¼ncellendi")
                    except:
                        logger.error(f"ğŸ’€ {crawler_name} DB gÃ¼ncelleme de baÅŸarÄ±sÄ±z!")

            return spider_closed_callback

        # XmlParser crawler'Ä±nÄ± ekle
        logger.info("ğŸ”§ XmlParser ekleniyor...")
        process.crawl(
            XmlParser,
            product_service=product_service,
            screenshot_service=screenshot_service,
            crawler_log_id=xml_parser_log_id,
            crawler_log_repo=crawler_log_repo
        )
        logger.info("âœ… XmlParser crawler'Ä± eklendi")

        # PriceParser crawler'Ä±nÄ± ekle
        logger.info("ğŸ”§ PriceParser ekleniyor...")
        process.crawl(
            PriceParser,
            product_service=product_service,
            screenshot_service=screenshot_service,
            crawler_log_id=price_parser_log_id,
            crawler_log_repo=crawler_log_repo
        )
        logger.info("âœ… PriceParser crawler'Ä± eklendi")

        # Callback'leri baÄŸla
        logger.info("ğŸ”— Callback'ler baÄŸlanÄ±yor...")
        callback_connected_count = 0

        for crawler in process.crawlers:
            if crawler.spidercls == XmlParser:
                xml_callback = create_spider_callback(xml_parser_log_id, "XmlParser")
                crawler.signals.connect(xml_callback, signal=signals.spider_closed)
                callback_connected_count += 1
                logger.info("âœ… XmlParser Callback baÅŸarÄ±yla baÄŸlandÄ±")

            elif crawler.spidercls == PriceParser:
                price_callback = create_spider_callback(price_parser_log_id, "PriceParser")
                crawler.signals.connect(price_callback, signal=signals.spider_closed)
                callback_connected_count += 1
                logger.info("âœ… PriceParser Callback baÅŸarÄ±yla baÄŸlandÄ± (detaylÄ± stats ile)")

        logger.info(f"ğŸ“Š Toplam {callback_connected_count}/2 callback baÄŸlandÄ±")

        if callback_connected_count != 2:
            logger.warning("âš ï¸ TÃ¼m callback'ler baÄŸlanamadÄ±! Spider'larÄ±n kendi closed() metodlarÄ± kullanÄ±lacak")

        # Process'i baÅŸlat
        logger.info("ğŸš€ Process baÅŸlatÄ±lÄ±yor (2 crawler birlikte)...")
        process.start(stop_after_crawl=True)
        logger.info("ğŸ‰ Process baÅŸarÄ±yla tamamlandÄ±")

    except Exception as e:
        logger.error(f"ğŸ’¥ Process hatasÄ±: {repr(e)}")
        logger.error(f"ğŸ“‹ Process hata detaylarÄ±: {traceback.format_exc()}")

        # Hata durumunda BOTH log kayÄ±tlarÄ±nÄ± gÃ¼ncelle
        if crawler_log_repo:
            if xml_parser_log_id:
                try:
                    crawler_log_repo.fail_crawler(xml_parser_log_id, str(e))
                    logger.info("âš ï¸ XmlParser hata durumunda gÃ¼ncellendi")
                except Exception as db_error:
                    logger.error(f"ğŸ’€ XmlParser DB gÃ¼ncelleme baÅŸarÄ±sÄ±z: {repr(db_error)}")

            if price_parser_log_id:
                try:
                    crawler_log_repo.fail_crawler(price_parser_log_id, str(e))
                    logger.info("âš ï¸ PriceParser hata durumunda gÃ¼ncellendi")
                except Exception as db_error:
                    logger.error(f"ğŸ’€ PriceParser DB gÃ¼ncelleme baÅŸarÄ±sÄ±z: {repr(db_error)}")

    finally:
        if db_session:
            try:
                db_session.close()
                logger.info("âœ… VeritabanÄ± oturumu kapatÄ±ldÄ±")
            except Exception as close_error:
                logger.error(f"âš ï¸ DB oturum kapatma hatasÄ±: {repr(close_error)}")


if __name__ == "__main__":
    main()