import time
from typing import Any
import scrapy
from scrapy.http import XmlResponse
from app.services.ProductService import ProductService
from app.services.ScreenshotService import ScreenshotService


class XmlParser(scrapy.Spider):
    name = 'xml-feed-parser'
    start_urls = ['https://www.pt.com.tr/wp-content/uploads/wpwoof-feed/xml/iprice.xml']

    # PERFORMANS AYARLARI - Ã–NEMLÄ°!
    custom_settings = {
        'CONCURRENT_REQUESTS': 64,
        'CONCURRENT_REQUESTS_PER_DOMAIN': 32,
        'DOWNLOAD_DELAY': 0,
        'RANDOMIZE_DOWNLOAD_DELAY': False,
        'RETRY_ENABLED': False,
        'REDIRECT_ENABLED': False,
        'COOKIES_ENABLED': False,
        'AUTOTHROTTLE_ENABLED': True,
        'AUTOTHROTTLE_START_DELAY': 0,
        'AUTOTHROTTLE_MAX_DELAY': 0.5,
        'AUTOTHROTTLE_TARGET_CONCURRENCY': 16.0,
    }

    def __init__(
            self,
            product_service: ProductService,
            screenshot_service: ScreenshotService,
            crawler_log_id=None,
            crawler_log_repo=None,
            **kwargs: Any,
    ):
        super().__init__(**kwargs)
        self.db_session = None
        self.product_service = product_service
        self.screenshot_service = screenshot_service
        self.crawler_log_id = crawler_log_id
        self.crawler_log_repo = crawler_log_repo

        # URL sayaÃ§larÄ±
        self.processed_products_count = 0
        self.total_urls_visited = 0
        self.xml_urls_count = 0
        self.product_urls_count = 0

        # FIX 1: Web price batch initialize et!
        self.web_price_batch = []  # None deÄŸil, boÅŸ liste!
        self.batch_size = 50  # None deÄŸil, sayÄ±!

    def parse(self, response: XmlResponse, **kwargs):
        ns = {
            'g': 'http://base.google.com/ns/1.0'
        }

        # XML URL sayacÄ±nÄ± arttÄ±r
        self.xml_urls_count += 1
        self.total_urls_visited += 1
        self.logger.info(f"ğŸŒ XML URL iÅŸlendi: {response.url}")

        items = response.xpath('//item')
        total_items = len(items)
        self.logger.info(f"ğŸ“¦ XML'den {total_items} Ã¼rÃ¼n bulundu")

        # SÃœPER HIZLI TRUNCATE - History korunur
        self.logger.info("âš¡ TRUNCATE ile sÃ¼per hÄ±zlÄ± silme...")
        start_delete = time.time()

        self.product_service.delete_all_products_truncate()

        end_delete = time.time()
        self.logger.info(f"ğŸš€ TRUNCATE tamamlandÄ±: {end_delete - start_delete:.2f} saniye")

        batch_products = []
        batch_images = []
        batch_histories = []

        self.logger.info("âš¡ BATCH MODE: TÃ¼m Ã¼rÃ¼nler hafÄ±zada toplanÄ±yor...")

        for i, item in enumerate(items, 1):
            if i % 100 == 0:  # Her 100 Ã¼rÃ¼nde progress
                self.logger.info(f"âš™ï¸ HazÄ±rlanan Ã¼rÃ¼n: {i}/{total_items}")

            product_data = {
                'title': item.xpath('g:title/text()',namespaces=ns).get(),
                'mpn': item.xpath('g:mpn/text()', namespaces=ns).get(),
                'gtin': item.xpath('g:gtin/text()', namespaces=ns).get(),
                'availability': item.xpath('g:availability/text()', namespaces=ns).get(),
                'price': item.xpath('g:price/text()', namespaces=ns).get(),
                'sale_price': item.xpath('g:sale_price/text()', namespaces=ns).get(),
                'condition': item.xpath('g:condition/text()', namespaces=ns).get(),
                'description': item.xpath('g:description/text()', namespaces=ns).get(),
                'brand': item.xpath('g:brand/text()', namespaces=ns).get(),
                'link': item.xpath('g:link/text()', namespaces=ns).get(),
                'product_type': item.xpath('g:product_type/text()', namespaces=ns).get(),
                'product_status': item.xpath('g:custom_label_4/text()', namespaces=ns).get(),
            }

            batch_products.append(product_data)

            image_url = item.xpath('g:image_link/text()', namespaces=ns).get()
            additional_image_links = item.xpath('g:additional_image_link/text()', namespaces=ns).getall()

            all_image_links = []
            if image_url:
                all_image_links.append(image_url)
            if additional_image_links:
                all_image_links.extend(additional_image_links)

            if all_image_links:
                for url in all_image_links:
                    if url:
                        batch_images.append({
                            'url': url,
                            'mpn': product_data['mpn']
                        })

            # History batch'e ekle
            batch_histories.append({
                'product_data': product_data,
                'source': 'xml_cron'
            })

        self.logger.info(f"ğŸ’¾ BULK INSERT baÅŸlÄ±yor: {len(batch_products)} Ã¼rÃ¼n...")
        start_time = time.time()

        saved_count = self.product_service.save_products_bulk(
            batch_products,
            batch_images,
            batch_histories
        )

        end_time = time.time()
        self.processed_products_count = saved_count

        self.logger.info(f"âœ… BULK INSERT tamamlandÄ±!")
        self.logger.info(f"   - Kaydedilen Ã¼rÃ¼n: {saved_count}")
        self.logger.info(f"   - SÃ¼re: {end_time - start_time:.2f} saniye")
        self.logger.info(f"   - HÄ±z: {saved_count / (end_time - start_time):.0f} Ã¼rÃ¼n/saniye")

        self.logger.info("ğŸš€ Ä°kinci aÅŸama: Web fiyat gÃ¼ncellemesi baÅŸlÄ±yor...")
        for request in self.update_web_price():
            yield request

    def update_web_price(self):
        """Web fiyatlarÄ±nÄ± gÃ¼ncelle - SÃœPER HIZLI"""
        all_products = self.product_service.get_all_products()

        # FIX 2: GeÃ§erli URL filtreleme geri ekle!
        valid_products = [p for p in all_products if p.link and p.link.startswith('http')]

        self.logger.info(f"ğŸ”„ PARALEL web fiyat gÃ¼ncellemesi: {len(valid_products)} Ã¼rÃ¼n")
        self.logger.info(f"âš¡ Concurrent requests: 64 (Ã§ok hÄ±zlÄ±)")

        # FIX 3: Sadece geÃ§erli URL'lere git!
        for product in valid_products:
            yield scrapy.Request(
                url=product.link,
                callback=self.get_price_from_link_and_update,
                meta={'product': product},
                dont_filter=True,
                errback=self.handle_error,
                priority=1
            )

    def get_price_from_link_and_update(self, response):
        """Web fiyatÄ±nÄ± al ve BATCH'e ekle"""
        try:
            self.product_urls_count += 1
            self.total_urls_visited += 1

            # Her 50'de bir progress
            if self.product_urls_count % 50 == 0:
                self.logger.info(f"ğŸŒ Ä°ÅŸlenen URL: {self.product_urls_count}")

            # Fiyat parse et
            web_price = (
                    response.xpath(
                        '(//span[contains(@class, "woocommerce-Price-amount amount")])[2]//bdi/text()').get() or
                    response.xpath('//span[contains(@class, "woocommerce-Price-amount")]//bdi/text()').get()
            )

            my_product = response.meta['product']

            # BATCH'e ekle - tek tek DB'ye yazma!
            self.web_price_batch.append({
                'product': my_product,
                'web_price': web_price,
                'success': web_price is not None
            })

            # Batch dolu ise kaydet
            if len(self.web_price_batch) >= self.batch_size:
                self._save_web_price_batch()

        except Exception as e:
            self.logger.error(f"ğŸ’¥ Fiyat gÃ¼ncelleme hatasÄ±: {e}")

    def _save_web_price_batch(self):
        """Web fiyat batch'ini toplu olarak kaydet"""
        if not self.web_price_batch:
            return

        try:
            # Batch'i toplu olarak kaydet
            success_count = self.product_service.update_web_prices_bulk(self.web_price_batch)

            self.logger.info(f"ğŸ’¾ Batch kaydedildi: {success_count}/{len(self.web_price_batch)} baÅŸarÄ±lÄ±")

            # Batch'i temizle
            self.web_price_batch.clear()

        except Exception as e:
            self.logger.error(f"ğŸ’¥ Batch kaydetme hatasÄ±: {e}")
            self.web_price_batch.clear()

    def handle_error(self, failure):
        """Hata yakalama fonksiyonu"""
        self.logger.error(f"ğŸ’¥ Request hatasÄ±: {failure.value}, URL: {failure.request.url}")

    def closed(self, reason):
        """Spider kapandÄ±ÄŸÄ±nda otomatik Ã§alÄ±ÅŸÄ±r"""
        try:
            # FIX 4: Kalan batch'i kaydet!
            if self.web_price_batch:
                self.logger.info("ğŸ“ Son batch kaydediliyor...")
                self._save_web_price_batch()

            self.logger.info(f"=== ğŸ•·ï¸ SPIDER CLOSED METHOD Ã‡ALIÅTI ===")
            self.logger.info(f"Spider: {self.name}, Reason: {reason}")
            self.logger.info(f"ğŸ“Š TOPLAM URL Ä°STATÄ°STÄ°KLERÄ°:")
            self.logger.info(f"   - XML URL'leri: {self.xml_urls_count}")
            self.logger.info(f"   - ÃœrÃ¼n URL'leri: {self.product_urls_count}")
            self.logger.info(f"   - Toplam URL: {self.total_urls_visited}")
            self.logger.info(f"   - Ä°ÅŸlenen Ã¼rÃ¼n: {self.processed_products_count}")

            if self.crawler_log_id and self.crawler_log_repo:
                # Stats al
                stats = self.crawler.stats.get_stats()

                # Kendi sayaÃ§larÄ±mÄ±zÄ± ekle
                crawler_stats = {
                    'processed': stats.get('response_received_count', 0),
                    'updated': self.product_urls_count,
                    'created': self.processed_products_count,
                    'downloader/request_count': stats.get('downloader/request_count', 0),
                    'downloader/response_count': stats.get('downloader/response_count', 0),
                    'downloader/response_status_count/200': stats.get('downloader/response_status_count/200', 0),
                    'downloader/response_status_count/404': stats.get('downloader/response_status_count/404', 0),
                    'elapsed_time_seconds': stats.get('elapsed_time_seconds', 0),

                    # CUSTOM STATÄ°STÄ°KLER
                    'custom/xml_urls_count': self.xml_urls_count,
                    'custom/product_urls_count': self.product_urls_count,
                    'custom/total_urls_visited': self.total_urls_visited,
                    'custom/processed_products_count': self.processed_products_count,
                }

                self.logger.info(f"ğŸ“ˆ DETAYLI Ä°STATÄ°STÄ°KLER:")
                self.logger.info(f"   - Scrapy total request: {crawler_stats['downloader/request_count']}")
                self.logger.info(f"   - Scrapy total response: {crawler_stats['downloader/response_count']}")
                self.logger.info(f"   - BaÅŸarÄ±lÄ± (200): {crawler_stats['downloader/response_status_count/200']}")
                self.logger.info(f"   - Hata (404): {crawler_stats['downloader/response_status_count/404']}")
                self.logger.info(f"   - SÃ¼re: {crawler_stats['elapsed_time_seconds']} saniye")

                # DB gÃ¼ncelle
                if reason == 'finished':
                    import json
                    stats_json = json.dumps(crawler_stats, indent=2)
                    self.crawler_log_repo.complete_crawler(self.crawler_log_id, {
                        'processed': crawler_stats['downloader/response_count'],
                        'updated': self.product_urls_count,
                        'created': self.processed_products_count,
                        'stats_json': stats_json
                    })
                    self.logger.info("âœ… DB baÅŸarÄ±yla gÃ¼ncellendi (finished)")
                else:
                    self.crawler_log_repo.fail_crawler(self.crawler_log_id, f"Spider kapatÄ±ldÄ±: {reason}")
                    self.logger.info("âŒ DB baÅŸarÄ±yla gÃ¼ncellendi (failed)")
            else:
                self.logger.warning("âš ï¸ crawler_log_id veya crawler_log_repo None!")

        except Exception as e:
            self.logger.error(f"ğŸ’¥ Spider closed method hatasÄ±: {repr(e)}")
            import traceback
            self.logger.error(f"ğŸ“‹ Traceback: {traceback.format_exc()}")